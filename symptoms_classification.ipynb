{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "928ba878",
   "metadata": {},
   "source": [
    "<h3>Library Imports</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c5219ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from string import punctuation\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7167a522",
   "metadata": {},
   "source": [
    "<h3>Loading text data of quality defect comments and symptoms</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51c2ea06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_type.csv')\n",
    "df = df.astype('string')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da5f592",
   "metadata": {},
   "source": [
    "This is the overall dataset available for training and testing the model. The dataset is not large enough to train the model to classify all symptom types, as many classes have few samples which if used for training will render model more biased towards the first three classes, especially biased towards the first class - **\"5 - Quality Issue - Assembly\".**\n",
    "\n",
    "In cases where sufficient data size for training is not available, Data Augmentation can be used to augment data for training NLP models. With some techniques, a dataset with small size can be utlized to create more data for training. Some of the techniques are\n",
    "1. Synonymn Replacement\n",
    "2. Back Translation\n",
    "3. Bigram Flipping\n",
    "4. Replacing Entities\n",
    "5. Adding Noise to Data\n",
    "\n",
    "For this particular project, first three classes with the largest samples are considered for this training to keep things simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e00bc05b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5 - Quality Issue - Assembly      5011\n",
       "2 - Quality Issue - Appearance    1594\n",
       "4 - Quality Issue - Functional    1016\n",
       "Name: Symptom Type, dtype: Int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Symptom Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "766c2b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df = df[df['Symptom Type'].isin(['5 - Quality Issue - Assembly','2 - Quality Issue - Appearance','4 - Quality Issue - Functional'])]\n",
    "x = df['Reporter Comment']\n",
    "y = df['Symptom Type']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c79e02",
   "metadata": {},
   "source": [
    "<h3>Text Pre-Processing</h3>\n",
    "To prepare the text for training and testing model, the following needs to be performed:\n",
    "\n",
    "1. Tokenization. Splitting the setences or text into words.\n",
    "2. Lowercasing the words.\n",
    "3. Removing stopwords\n",
    "4. Removing punctuations and digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "885d1255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processor(texts):\n",
    "    mystopwords = set(stopwords.words(\"english\"))\n",
    "    def remove_stpwrd_punc_dig(tokens):\n",
    "        return [token.lower() for token in tokens if token.lower() not in mystopwords and not token.isdigit() and token not in punctuation]\n",
    "    return [remove_stpwrd_punc_dig(word_tokenize(text)) for text in texts]\n",
    "\n",
    "processed_x = text_processor(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dbc7ff",
   "metadata": {},
   "source": [
    "Here is the breakdown of training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f8ffb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of processed text:  7621 \n",
      "Size of classes:  7621\n",
      "\n",
      "Classes and their sizes:\n",
      " 5 - Quality Issue - Assembly      5011\n",
      "2 - Quality Issue - Appearance    1594\n",
      "4 - Quality Issue - Functional    1016\n",
      "Name: Symptom Type, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "print('Size of processed text: ',len(processed_x), '\\nSize of classes: ', len(y))\n",
    "print('\\nClasses and their sizes:\\n', y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c316be1",
   "metadata": {},
   "source": [
    "<h3>Feature Engineering/Text Representation with Word Embeddings</h3>\n",
    "\n",
    "The pre-processed text is represented in a numerical form so that it can be fed to a machine learning (ML) algorithm, and this step is referred to as text representation in NLP. More often in NLP, an optimal text representation yields far greater results even when used with an ordinary ML algorithm.\n",
    "\n",
    "There are different techniques available for performing text representations in NLP. Some of them that I am faimiliar with are\n",
    "1. Basic Vectorization of Text\n",
    "2. Word Embeddings\n",
    "\n",
    "One way to perform text representation is through basic vector representation of pre-processed text data, such using One-Hote encoding, Bag of Words, Bag of N-Grams, N-Grams, and Term Frequency-Inverse Document Frequency (TF-IDF). However, using basic vectorization of text come with drawbacks:\n",
    "- words are treated as atomic units, so relationship among them cannot be established\n",
    "- features vector size and sparisty incraeses which makes NLP model computationally expensive and can cause overfitting\n",
    "- the NLP model cannot handle out of vocabulary words\n",
    "\n",
    "Another way is to use Word Embeddings, which utlize the concept of Distributional Representations. Here the goal is to come up with a text representation that enables the model to derive meaning of the words from their contexts. A Data Scientist can train her/his own word embeddings for text representation or load existing, pre-trained word embeddings. Some popular pre-trained word embeddings are Word2Vec by Google, fasttext embeddings by Facebook, and GloVe word embeddings by Stanford University.\n",
    "\n",
    "For this particular task in hand, I will use pre-trained Word2Vec model. When creating this model, researchers came up with two architecures that could be used to train this model - Continous Bag of Words (CBOW) and SkipGram. The architectures are similar. With both architectures, the basic idea is to generate small size (between 25 to 600) numerical feature vectors for each word in the corpus, and use those feature vectors as means of comparing with other words in the corpus. Cosine similarity is generally used to compare between the words (feature vectors). Although there are some differences between CBOW and SkipGram, but the main conceptual difference is that in SkipGram a center word is used to predict its surrounding words; whereas, in CBOW, the surrounding (context) words are used to predict the center word.\n",
    "\n",
    "Word Embeddings, whether pre-trained or trained, also come with drawbacks. Two common drawbacks are:\n",
    "1. Word embeddings model size is large which makes the NLP model difficult to deploy. As a student of Computer Science, it will be a challenge for me to deploy an NLP model that used word2vec embeddings into Heroku's free tier server. \n",
    "\n",
    "2. Out of vocabulary issue as mentioned above\n",
    "\n",
    "As shown below, word2vec model is being loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "36110dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b12546",
   "metadata": {},
   "source": [
    "Below is an example cosine similarities of the most similar words to 'tesla' are displayed using the word2vec model. As can be seen, the context words for tesla are related to physics or electricity. This points out that the corpus used for modelling the Word2Vec embeddings did not contain or had very much less information related to the car manufacturing company, Tesla.\n",
    "\n",
    "**Note-** All words shown here, including 'universe', are present in the model's corpus, which means these words were present in the corpus that used for creating the Word2Vec embeddings model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6e18dcab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gauss', 0.6623970866203308),\n",
       " ('FT_ICR', 0.5639052391052246),\n",
       " ('MeV', 0.5619181990623474),\n",
       " ('keV', 0.5605965256690979),\n",
       " ('superconducting_magnet', 0.5567352175712585),\n",
       " ('electron_volt', 0.5503560900688171),\n",
       " ('SQUIDs', 0.5393732786178589),\n",
       " ('nT', 0.5386143326759338),\n",
       " ('electronvolts', 0.5377055406570435),\n",
       " ('kelvin', 0.5367920994758606)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar('tesla')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab4dab3",
   "metadata": {},
   "source": [
    "**If a particular word is not present in Word2Vec model corpus then an 'Key not present' error will display!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4970d40d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'AWordNotPresentInTheModelCorpus' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mw2v_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAWordNotPresentInTheModelCorpus\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive - MNSCU\\Projects\\2022_4_NLP_Symptom Identification\\nlp\\lib\\site-packages\\gensim\\models\\keyedvectors.py:773\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    771\u001b[0m     mean\u001b[38;5;241m.\u001b[39mappend(weight \u001b[38;5;241m*\u001b[39m key)\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 773\u001b[0m     mean\u001b[38;5;241m.\u001b[39mappend(weight \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[0;32m    774\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_index_for(key):\n\u001b[0;32m    775\u001b[0m         all_keys\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_index(key))\n",
      "File \u001b[1;32m~\\OneDrive - MNSCU\\Projects\\2022_4_NLP_Symptom Identification\\nlp\\lib\\site-packages\\gensim\\models\\keyedvectors.py:438\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[1;34m(self, key, norm)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vector\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    415\u001b[0m     \u001b[38;5;124;03m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \n\u001b[0;32m    417\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    436\u001b[0m \n\u001b[0;32m    437\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 438\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m norm:\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_norms()\n",
      "File \u001b[1;32m~\\OneDrive - MNSCU\\Projects\\2022_4_NLP_Symptom Identification\\nlp\\lib\\site-packages\\gensim\\models\\keyedvectors.py:412\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[1;34m(self, key, default)\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 412\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'AWordNotPresentInTheModelCorpus' not present\""
     ]
    }
   ],
   "source": [
    "w2v_model.most_similar('AWordNotPresentInTheModelCorpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37592d7b",
   "metadata": {},
   "source": [
    "Feature vector of a word present in model's corpus can be acquired. The code below shows an example of getting feature vector of the word 'tesla'. Also, the feature vector size was set to 300 during Word2Vec development, which results in all feature vectors of words in corpus to have same size feature vector of 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "49cdea73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.74804688e-01,  1.25000000e-01,  5.66406250e-01,  2.58789062e-02,\n",
       "        5.12695312e-03,  1.27929688e-01,  3.02734375e-01, -7.03125000e-01,\n",
       "       -9.27734375e-02, -1.30859375e-01, -4.80957031e-02,  1.18164062e-01,\n",
       "        2.83203125e-01,  1.40625000e-01,  1.29882812e-01,  4.19921875e-01,\n",
       "       -3.32031250e-01,  2.96875000e-01, -2.41088867e-03, -1.51367188e-02,\n",
       "       -3.02734375e-01, -1.54296875e-01,  2.38037109e-02, -3.24707031e-02,\n",
       "       -2.09960938e-01, -3.35937500e-01, -3.06640625e-01, -5.95092773e-03,\n",
       "       -3.75000000e-01, -2.91015625e-01, -7.03125000e-02, -1.39648438e-01,\n",
       "       -2.73437500e-01,  9.76562500e-03, -1.70898438e-01, -8.05664062e-02,\n",
       "       -3.10546875e-01,  3.32031250e-01,  7.62939453e-03,  2.10937500e-01,\n",
       "        3.08593750e-01,  2.75390625e-01,  1.31835938e-01,  2.45117188e-01,\n",
       "       -6.95800781e-03, -1.35742188e-01,  1.48437500e-01,  2.91748047e-02,\n",
       "       -3.18359375e-01, -2.75878906e-02, -8.59375000e-02, -2.27539062e-01,\n",
       "       -1.18408203e-02,  7.62939453e-04,  3.61328125e-01,  2.55859375e-01,\n",
       "        5.24902344e-02, -1.89453125e-01,  1.51367188e-01,  3.68652344e-02,\n",
       "       -3.12500000e-02, -4.04296875e-01,  2.75390625e-01, -2.50000000e-01,\n",
       "        3.67187500e-01, -8.34960938e-02, -4.25781250e-01,  2.21679688e-01,\n",
       "        1.89208984e-02,  1.43554688e-01, -2.94189453e-02, -8.93554688e-02,\n",
       "       -1.55273438e-01, -2.92968750e-01, -2.55859375e-01, -2.50244141e-02,\n",
       "        8.00781250e-02, -3.55468750e-01, -3.61328125e-02, -1.48437500e-01,\n",
       "        7.37304688e-02,  3.06640625e-01,  9.71679688e-02,  2.42187500e-01,\n",
       "        9.81445312e-02,  2.73437500e-01,  1.97265625e-01,  2.91015625e-01,\n",
       "       -1.12792969e-01, -7.42187500e-02, -1.86523438e-01,  3.07617188e-02,\n",
       "        2.35351562e-01,  3.14453125e-01, -3.26171875e-01,  9.66796875e-02,\n",
       "        2.29492188e-01, -2.55859375e-01,  5.07812500e-01, -1.99218750e-01,\n",
       "        3.45703125e-01, -1.93359375e-01,  7.95898438e-02, -2.47070312e-01,\n",
       "        2.89062500e-01,  8.74023438e-02, -3.20312500e-01,  4.12109375e-01,\n",
       "        6.78710938e-02,  2.83203125e-02,  7.08007812e-02,  5.43212891e-03,\n",
       "        4.78515625e-01,  2.49023438e-01, -4.00390625e-02,  2.96875000e-01,\n",
       "       -2.87109375e-01, -1.24511719e-01,  2.17773438e-01, -6.13281250e-01,\n",
       "       -7.22656250e-02, -1.07421875e-01,  2.32421875e-01, -8.54492188e-03,\n",
       "       -5.34667969e-02, -2.07519531e-02,  3.80859375e-01, -7.10487366e-05,\n",
       "       -2.00195312e-01,  1.66015625e-01,  4.10156250e-01, -3.44238281e-02,\n",
       "       -1.37329102e-02, -2.67578125e-01,  1.59179688e-01, -4.68750000e-02,\n",
       "       -1.12792969e-01, -1.27929688e-01, -6.59179688e-02, -2.44140625e-02,\n",
       "        5.61523438e-02, -1.12304688e-02, -9.94873047e-03,  2.00195312e-01,\n",
       "       -7.56835938e-02, -4.05883789e-03,  2.88085938e-02, -2.73437500e-01,\n",
       "       -3.51562500e-02, -3.76953125e-01,  7.03125000e-02, -6.95312500e-01,\n",
       "       -3.10546875e-01, -1.80664062e-01, -1.87988281e-02,  4.15039062e-02,\n",
       "        1.84570312e-01, -8.20312500e-02,  1.10351562e-01, -1.38671875e-01,\n",
       "       -1.06933594e-01,  1.07421875e-01,  4.80957031e-02,  9.64355469e-03,\n",
       "        2.65625000e-01, -2.81250000e-01,  3.30078125e-01, -2.40234375e-01,\n",
       "       -4.21875000e-01,  2.19726562e-02,  2.51770020e-03, -1.25976562e-01,\n",
       "        3.02734375e-02,  2.84423828e-02, -1.33789062e-01,  1.31835938e-01,\n",
       "        1.22680664e-02,  3.14941406e-02, -1.79687500e-01, -3.32031250e-01,\n",
       "       -2.48046875e-01,  2.32421875e-01, -3.82812500e-01,  1.09375000e-01,\n",
       "       -5.46875000e-02,  3.08837891e-02,  5.68847656e-02, -1.64062500e-01,\n",
       "       -2.23388672e-02,  6.25000000e-02, -1.22070312e-02,  2.51953125e-01,\n",
       "       -5.73730469e-03,  2.76184082e-03, -1.38549805e-02,  2.08007812e-01,\n",
       "       -6.54296875e-02,  4.30297852e-03,  1.14746094e-01,  5.20019531e-02,\n",
       "       -2.08007812e-01,  5.54687500e-01, -1.19018555e-02,  1.51367188e-01,\n",
       "       -1.09375000e-01, -5.07812500e-01, -1.94335938e-01, -4.61425781e-02,\n",
       "       -5.41992188e-02,  2.39257812e-01, -7.51495361e-04, -2.46093750e-01,\n",
       "       -1.40625000e-01, -1.15966797e-02, -4.25781250e-01,  1.22558594e-01,\n",
       "       -2.61230469e-02,  9.61914062e-02, -4.90722656e-02,  2.16796875e-01,\n",
       "       -2.18750000e-01,  1.64062500e-01,  1.58203125e-01,  8.39843750e-02,\n",
       "        5.39550781e-02,  1.98242188e-01,  7.76367188e-02,  3.28125000e-01,\n",
       "        1.17675781e-01, -2.77343750e-01, -4.00390625e-01, -1.41601562e-01,\n",
       "       -1.14257812e-01,  1.22070312e-02, -7.76367188e-02,  2.11914062e-01,\n",
       "       -1.15234375e-01, -1.92382812e-01,  2.00195312e-01,  1.24023438e-01,\n",
       "        3.02734375e-01, -1.62109375e-01,  6.34765625e-02, -6.05468750e-02,\n",
       "       -4.93164062e-02,  2.33398438e-01, -6.25000000e-02,  3.39843750e-01,\n",
       "        1.06933594e-01, -6.78710938e-02,  5.93261719e-02, -3.76953125e-01,\n",
       "       -8.10546875e-02,  9.57031250e-02,  2.83203125e-01, -6.93359375e-02,\n",
       "        1.97753906e-02, -9.86328125e-02, -7.71484375e-02,  2.18750000e-01,\n",
       "        1.99218750e-01, -7.12890625e-02, -1.12304688e-01, -1.83593750e-01,\n",
       "       -1.09863281e-02,  2.36328125e-01,  1.15722656e-01, -3.59375000e-01,\n",
       "        3.14941406e-02, -6.83593750e-02, -2.42187500e-01, -3.33984375e-01,\n",
       "       -1.49414062e-01, -2.01171875e-01,  1.12792969e-01,  7.91015625e-02,\n",
       "       -3.14453125e-01, -2.39257812e-01, -3.73046875e-01,  1.78710938e-01,\n",
       "       -1.21093750e-01,  4.08203125e-01, -9.81445312e-02,  4.32128906e-02,\n",
       "        4.76074219e-02, -2.77343750e-01,  2.28515625e-01, -5.03906250e-01,\n",
       "       -1.19628906e-01,  1.22558594e-01, -1.49414062e-01,  5.62500000e-01,\n",
       "       -5.54687500e-01,  6.22558594e-02, -4.32128906e-02,  8.78906250e-02,\n",
       "       -3.96484375e-01,  1.34765625e-01,  2.90527344e-02,  3.43750000e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(w2v_model['tesla']))\n",
    "w2v_model['tesla']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d9950a",
   "metadata": {},
   "source": [
    "<h4>Function to acquire feature vectors of words in the pro-processed training corpus</h4>\n",
    "\n",
    "The **embedding_features** function is to acquire feature vectors of words in the pro-processed training corpus. The function goes through each word in training corpus and verifies whether the word is present in embeddings model. If a word exists in the w2v_model then its feature vector is generated. For a single sentence, feature vectors of all words are averaged to represent an average feature vector of each sentence in the training corpus.\n",
    "\n",
    "As mentioned above, one drawback of Word2Vec model is the out of vocabulary issue. In this function below, any word in the training corpus that is not present in the w2v_model is discarded. Therefore, when using a pre-trained embeddings model it is important to keep in mind the domain under which the embeddings model was trained. Word2Vec was trained using large corpus from Google News; however, my training dataset is a corpus from manufacturing, which not ideal. A pre-trained word embeddings model trained using manufacturing text corpus would yeild a much higher end result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f97c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_features(list_of_lists):\n",
    "    DIMENSION = 300\n",
    "    zero_vector = np.zeros(DIMENSION)\n",
    "    features = []\n",
    "    for tokens in list_of_lists:\n",
    "        count_for_this = 0 + 1e-5\n",
    "        feat_for_this = np.zeros(DIMENSION)\n",
    "        for token in tokens:\n",
    "            if token in w2v_model:\n",
    "                feat_for_this += w2v_model[token]\n",
    "                count_for_this += 1\n",
    "        if (count_for_this != 0):\n",
    "            features.append(feat_for_this/count_for_this)\n",
    "        else:\n",
    "            features.append(zero_vector)\n",
    "    return features\n",
    "\n",
    "features = embedding_features(processed_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f276578",
   "metadata": {},
   "source": [
    "Although the size of processed_x (mentioned few cells above) and features are same, 7621, within each list (pre-processed sentence) of processed_x, some words have been eliminated because of **domain discrepancy** between manufacturing and news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b648abeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7621, 300)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = np.array(features)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29da2826",
   "metadata": {},
   "source": [
    "<h3>Random Forest Classifier for Prediction</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ee3fbbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take any classifier (LogisticRegression here, and train/test it like before.\n",
    "classifier = RandomForestClassifier()\n",
    "train_data, test_data, train_cats, test_cats = train_test_split(features, y)\n",
    "classifier.fit(train_data, train_cats)\n",
    "preds = classifier.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98313b54",
   "metadata": {},
   "source": [
    "<h3>Model Evaluation</h3>\n",
    "\n",
    "Model's f1-score with respect to **5 - Quality Issue - Assembly** is the highest compared to the other two categories. As assumed before due to imbalanced sample sizes of the categories, the classifier's prediction for this 'assembly' symptom was more precise (on average, out of 100 samples predicted as 'assembly', 96 of them were accurate/true positive) compared to its prediction for other two symptom's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bd4e86b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                precision    recall  f1-score   support\n",
      "\n",
      "2 - Quality Issue - Appearance       0.68      0.89      0.77       309\n",
      "4 - Quality Issue - Functional       0.29      0.80      0.42        89\n",
      "  5 - Quality Issue - Assembly       0.96      0.80      0.87      1508\n",
      "\n",
      "                      accuracy                           0.82      1906\n",
      "                     macro avg       0.64      0.83      0.69      1906\n",
      "                  weighted avg       0.89      0.82      0.84      1906\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(preds, test_cats))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
